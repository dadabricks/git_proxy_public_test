{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbff0c1-6c44-4350-a00a-b2d45f3eb9f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Enable Git Proxy for private Git server connectivity in Repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "435352ef-6679-46a3-bd2f-98f8328cfab9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview\n",
    "This private preview feature is available on AWS and Azure.\n",
    "\n",
    "**Note**: an *admin* must run this notebook to enable the feature.\n",
    "\n",
    "\"Run all\" this notebook to set up a cluster that proxies requests to your private Git server. Running this notebook does the following things:\n",
    "\n",
    "0. Writes a shell script to DBFS (`dbfs:/databricks/db_repos_proxy/db_repos_proxy_init.sh`) that is used as a [cluster-scoped init script](https://docs.databricks.com/clusters/init-scripts.html#example-cluster-scoped-init-scripts).\n",
    "0. Creates a [single node cluster](https://docs.databricks.com/clusters/single-node.html) named `dp_git_proxy` that runs the init script on start-up. **Important**: all users in the workspace will be granted \"attach to\" permissions to the cluster.\n",
    "0. Enables a feature flag that controls whether Git requests in Repos are proxied via the cluster.\n",
    "\n",
    "You may need to wait several minutes after running this notebook for the cluster to reach a \"RUNNING\" state. It can also take up to 30 minutes for the feature flag configuration to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26784ee9-fe81-4de5-9e40-00281bdad192",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Write Cluster Init Script to DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "454d16cf-b8f8-40ef-9469-27072392ab49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db_repos_proxy_init = \"\"\"\n",
    "#!/bin/bash\n",
    "set -x\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Install Python\n",
    "mkdir /databricks/db_repos_proxy\n",
    "python3 -m pip install --index-url https://test.pypi.org/simple/ --no-deps databricks-repo-proxy --upgrade\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Setup Systemd\n",
    "cat > /etc/systemd/system/db_repos_proxy.service <<EOF\n",
    "[Service]\n",
    "Type=simple\n",
    "ExecStart=/databricks/python3/bin/db_proxy\n",
    "StandardInput=null\n",
    "StandardOutput=file:/databricks/db_repos_proxy/daemon.log\n",
    "StandardError=file:/databricks/db_repos_proxy/daemon.log\n",
    "Restart=always\n",
    "RestartSec=1\n",
    "\n",
    "[Unit]\n",
    "Description=Git Proxy Service\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "EOF\n",
    "#--------------------------------------------------\n",
    "\n",
    "systemctl daemon-reload\n",
    "systemctl enable db_repos_proxy.service\n",
    "systemctl start db_repos_proxy.service\n",
    "\"\"\"  # db_repos_proxy_init_end\n",
    "\n",
    "location = \"/databricks/db_repos_proxy/db_repos_proxy_init.sh\"\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/db_repos_proxy/\")\n",
    "dbutils.fs.put(location, db_repos_proxy_init, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c358474a-2546-4ee5-866c-28f2ee89a5e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create the proxy cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8355357d-d653-4851-baf2-3c4369fe47f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"cluster-name\", \"\", \"Git Proxy Cluster Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904a8b8f-04af-4037-88c2-1e372b12df8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "admin_token = (\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    ")\n",
    "databricks_instance = (\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    ")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {admin_token}\"}\n",
    "\n",
    "# Clusters\n",
    "CLUSTERS_LIST_ENDPOINT = \"/api/2.0/clusters/list\"\n",
    "CLUSTERS_CREATE_ENDPOINT = \"/api/2.0/clusters/create\"\n",
    "CLUSTERS_LIST_NODE_TYPES_ENDPOINT = \"/api/2.0/clusters/list-node-types\"\n",
    "\n",
    "# Permissions\n",
    "UPDATE_PERMISSIONS_ENDPOINT = \"/api/2.0/permissions/clusters\"\n",
    "\n",
    "# Workspace Conf\n",
    "WORKSPACE_CONF_ENDPOINT = \"/api/2.0/workspace-conf\"\n",
    "\n",
    "# get name to use for cluster\n",
    "cluster_name = \"dp_git_proxy\"  # default name\n",
    "widget_value = dbutils.widgets.get(\"cluster-name\")\n",
    "workspace_conf_value = requests.get(\n",
    "    databricks_instance + WORKSPACE_CONF_ENDPOINT + \"?keys=gitProxyClusterName\",\n",
    "    headers=headers,\n",
    ").json()[\"gitProxyClusterName\"]\n",
    "print(f\"widget value: {widget_value}\")\n",
    "print(f\"workspace conf value: {workspace_conf_value}\")\n",
    "\n",
    "if widget_value:\n",
    "    cluster_name = widget_value\n",
    "elif workspace_conf_value:\n",
    "    cluster_name = workspace_conf_value\n",
    "print(f\"Using cluster name {cluster_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27130db7-fd39-49a5-91c7-6f23977954d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_cluster_data = {\n",
    "  \"cluster_name\": cluster_name,\n",
    "  \"spark_version\": \"10.4.x-scala2.12\",\n",
    "  \"num_workers\": 0,\n",
    "  \"autotermination_minutes\": 0,\n",
    "  \"spark_conf\": {\n",
    "      \"spark.databricks.cluster.profile\": \"singleNode\",\n",
    "      \"spark.master\": \"local[*]\",\n",
    "  },\n",
    "  \"custom_tags\": {\"ResourceClass\": \"SingleNode\"},\n",
    "  \"init_scripts\": {\n",
    "      \"dbfs\": {\"destination\": \"dbfs:/databricks/dp_git_proxy/dp_git_proxy_init.sh\"}\n",
    "  },\n",
    "}\n",
    "# get list of node types to determine whether this workspace is on AWS or Azure\n",
    "clusters_node_types = requests.get(\n",
    "  databricks_instance + CLUSTERS_LIST_NODE_TYPES_ENDPOINT, headers=headers\n",
    ").json()[\"node_types\"]\n",
    "node_type_ids = [type[\"node_type_id\"] for type in clusters_node_types]\n",
    "aws_node_type_id = \"m5.large\"\n",
    "azure_node_type_id = \"Standard_DS2_v2\"\n",
    "if aws_node_type_id in node_type_ids:\n",
    "  create_cluster_data = {\n",
    "      **create_cluster_data,\n",
    "      \"node_type_id\": aws_node_type_id,\n",
    "      \"aws_attributes\": {\"ebs_volume_count\": \"1\", \"ebs_volume_size\": \"32\"},\n",
    "  }\n",
    "elif azure_node_type_id in node_type_ids:\n",
    "  create_cluster_data = {**create_cluster_data, \"node_type_id\": azure_node_type_id}\n",
    "else:\n",
    "  raise ValueError(\n",
    "      f\"Node types {aws_node_type_id} or {azure_node_type_id} do not exist. Make sure you are on AWS or Azure, or contact support.\"\n",
    "  )\n",
    "\n",
    "# Note: this only returns up to 100 terminated all-purpose clusters in the past 30 days\n",
    "clusters_list_response = requests.get(\n",
    "  databricks_instance + CLUSTERS_LIST_ENDPOINT, headers=headers\n",
    ").json()\n",
    "clusters_list = clusters_list_response[\"clusters\"]\n",
    "clusters_names = [\n",
    "  cluster[\"cluster_name\"] for cluster in clusters_list_response[\"clusters\"]\n",
    "]\n",
    "print(f\"List of existing clusters: {clusters_names}\")\n",
    "\n",
    "if cluster_name in clusters_names:\n",
    "  raise ValueError(\n",
    "      f\"Cluster called {cluster_name} already exists. Please delete this cluster and re-run this notebook\"\n",
    "  )\n",
    "else:\n",
    "  # Create a new cluster named cluster_name that will proxy requests to the private Git server\n",
    "  print(f\"Create cluster POST request data: {create_cluster_data}\")\n",
    "  clusters_create_response = requests.post(\n",
    "      databricks_instance + CLUSTERS_CREATE_ENDPOINT,\n",
    "      headers=headers,\n",
    "      json=create_cluster_data,\n",
    "  ).json()\n",
    "  print(f\"Create cluster response: {clusters_create_response}\")\n",
    "  cluster_id = clusters_create_response[\"cluster_id\"]\n",
    "  print(f\"Created new cluster with id {cluster_id}\")\n",
    "  update_permissions_data = {\n",
    "      \"access_control_list\": [\n",
    "          {\"group_name\": \"users\", \"permission_level\": \"CAN_ATTACH_TO\"}\n",
    "      ]\n",
    "  }\n",
    "  update_permissions_response = requests.patch(\n",
    "      databricks_instance + UPDATE_PERMISSIONS_ENDPOINT + f\"/{cluster_id}\",\n",
    "      headers=headers,\n",
    "      json=update_permissions_data,\n",
    "  ).json()\n",
    "  print(f\"Update permissions response: {update_permissions_response}\")\n",
    "  print(f\"Gave all users ATTACH TO permissions to cluster {cluster_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de9417bd-06fb-4789-aac7-48d04083f241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Flip the feature flag!\n",
    "This flips the feature flag to route Git requests to the cluster. The change should take into effect within an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddb73af1-872c-469a-96e1-024cceea4d11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "patch_enable_git_proxy_data = {\"enableGitProxy\": \"true\"}\n",
    "patch_git_proxy_cluster_name_data = {\"gitProxyClusterName\": cluster_name}\n",
    "requests.patch(\n",
    "    databricks_instance + WORKSPACE_CONF_ENDPOINT,\n",
    "    headers=headers,\n",
    "    json=patch_enable_git_proxy_data,\n",
    ")\n",
    "requests.patch(\n",
    "    databricks_instance + WORKSPACE_CONF_ENDPOINT,\n",
    "    headers=headers,\n",
    "    json=patch_git_proxy_cluster_name_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b405bab-d390-4a4c-9c16-dc98481165f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Check that flag has been set\n",
    "If the command below returns with `{\"enableGitProxy\":\"true\"}`, you should be all set. Also, if you configured a custom cluster name using the widget, check that the cluster name in the response matches the name you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0dd822-c278-49f1-930f-e893e3e1acf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_flag_response = requests.get(\n",
    "  databricks_instance + WORKSPACE_CONF_ENDPOINT + \"?keys=enableGitProxy\",\n",
    "  headers=headers,\n",
    ").json()\n",
    "get_cluster_name_response = requests.get(\n",
    "  databricks_instance + WORKSPACE_CONF_ENDPOINT + \"?keys=gitProxyClusterName\",\n",
    "  headers=headers,\n",
    ").json()\n",
    "print(f\"Get enableGitProxy response: {get_flag_response}\")\n",
    "print(f\"Get gitProxyClusterName response: {get_cluster_name_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09cb2119-b112-4382-981b-1825017229b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Validation steps\n",
    "Attach this notebook to the **Git proxy cluster** that was just created and follow the instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6f391b-8628-42d8-8015-5fa97b285baa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "systemctl status db_repos_proxy.service\n",
    "journalctl -u db_repos_proxy.service\n",
    "cat /databricks/db_repos_proxy/daemon.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54eac9fe-760f-4d68-b08f-a31f29693cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1638105974359251,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "enable_git_proxy",
   "notebookOrigID": 1638105974357979,
   "widgets": {
    "cluster-name": {
     "currentValue": "",
     "nuid": "861bd512-2807-4279-ba01-ea211455eee8",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Git Proxy Cluster Name",
      "name": "cluster-name",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
